{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "**Name:** Functionalize ML pipeline  \n",
    "**Author:** Siyang Jing  \n",
    "**Organization:** UNC-CH  \n",
    "**License:** WTFPL  \n",
    "\n",
    "**Reference:**\n",
    "1. Tensorflow and Keras\n",
    "1. The author's other codes\n",
    "1. Relevant numerous papers\n",
    "  \n",
    "**Description:**  \n",
    "This file prepares a function ML pipeline based on ML_pipeline.ipynb. This code sets up the numerical experiment with only machine learning part. Input parameters of model, observation operator, ML architecture, range of value, and receive statistics.\n",
    "_Input_:\n",
    "* Parameters: (To be implemented)\n",
    "  * ndim: dimension of state space default is 40,\n",
    "  * pars: parameters for model ode, default value is (8.0) for Lorenz 96,\n",
    "  * nobs: number of observations, dafault is 50,\n",
    "  * deltaobs=0.1,\n",
    "  * dobs=None,\n",
    "  * Hmat=None,\n",
    "  * sigmaobs=0.9,\n",
    "  * infl_lin=0.5,\n",
    "  * infl_nlin=1,\n",
    "  * sigmainit=1.3,\n",
    "  * nens=100,\n",
    "  * ferr=1.0\n",
    "* DA Flags: (To be implemented)\n",
    "  * HTYPE: type of observation operator\n",
    "    * None: everything is specified in parameters, **TODO**\n",
    "    * 0: ObsOp_40_20\n",
    "    * 1: ObsOp_40_20 with Inv_20_10\n",
    "  * LINEAR: whether to use linearized KF or not, specific to $H$\n",
    "  * HERROR: whether H and Hm are the same\n",
    "* Usage Flags: (To be implemented)\n",
    "  * PLOTTING\n",
    "  * DEBUG\n",
    "  * SAVEDATA: indicator for which data to save  \n",
    "    * 0: Don't save anything\n",
    "    * 1: Save everything including,  \n",
    "      1. Flags and Parameters  \n",
    "      1. xfm, xam, xfcov, xacov  \n",
    "      1. yfm, yfcov  \n",
    "      1. statistics  \n",
    "    * 2: Flags, Paramters, Statistics (No running data)\n",
    "    * 3: haha\n",
    "  \n",
    "_Output_:  \n",
    "Forecast/analysis absolute error averaged over variables and time (from the 30th observation step as the starting point after DA stabilizes)\n",
    "1. xferravgxt: forecast absolute error averaged over all variables and time\n",
    "1. xaerravgxt: analysis absolute error averaged over all variables and time\n",
    "1. xferr10avgxt: forecast absolute error averaged over first 10 variables and time\n",
    "1. xaerr10avgxt: analysis absolute error averaged over first 10 variables and time\n",
    "1. xferr30avgxt: forecast absolute error averaged over last 30 variables and time\n",
    "1. xaerr30avgxt: analysis absolute error averaged over last 30 variables and time\n",
    "\n",
    "_Saved Files_:   \n",
    "Variables saved in a .npz file with their orginal names with numpy.savez method.  \n",
    "\n",
    "_Plots_:\n",
    "1. Long true trajectory\n",
    "1. Run results\n",
    "1. Statistics\n",
    "\n",
    "**Requirements:**\n",
    "1. Relevant Python modules\n",
    "1. AuxFuncs, which defines the following:  \n",
    "  1. Observation operator  \n",
    "  1. Stupid inverse function  \n",
    "  1. Lorenz96 as model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO List\n",
    "1. Needs a more sophisticated build_model function\n",
    "  * Input:\n",
    "    1. Layers\n",
    "    1. Optimizer\n",
    "  * Architecture selection\n",
    "1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from EnKF_func import *\n",
    "from AuxFuncs import *\n",
    "from ML_func import *\n",
    "from DataGenerator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT = 0.8\n",
    "NORMALIZE = False\n",
    "LOSS = 'mse'\n",
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "    keras.layers.Dense(20, activation=tf.nn.relu, \n",
    "                       input_shape=(40,)),\n",
    "    keras.layers.Dense(20, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(20)\n",
    "    ])\n",
    "\n",
    "    #optimizer = tf.train.RMSPropOptimizer(0.001)\n",
    "\n",
    "    model.compile(loss=LOSS,\n",
    "                optimizer='adam',\n",
    "                metrics=['mae','acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepareData(X,Y,\n",
    "                train_split=TRAIN_SPLIT,\n",
    "                normalize=NORMALIZE):\n",
    "    \n",
    "    data_size = np.shape(X)[0]\n",
    "\n",
    "    # Shuffle the input\n",
    "    order = np.argsort(np.random.random(data_size))\n",
    "\n",
    "    train_size = round(data_size * train_split)\n",
    "    test_size = data_size - train_size\n",
    "\n",
    "    train_data = X[order[0:train_size],:]\n",
    "    train_labels = Y[order[0:train_size],:]\n",
    "\n",
    "    test_data = X[order[train_size:],:]\n",
    "    test_labels = Y[order[train_size:],:]\n",
    "\n",
    "    if normalize:\n",
    "        ## Normalize the data\n",
    "        ## Doesn't seem particularly useful\n",
    "        # Test data is *not* used when calculating the mean and std.\n",
    "        '''\n",
    "        mean = train_data.mean(axis=0)\n",
    "        std = train_data.std(axis=0)\n",
    "        train_data = (train_data - mean) / std\n",
    "        test_data = (test_data - mean) / std\n",
    "        '''\n",
    "    \n",
    "    return (train_data,train_labels,\n",
    "            test_data,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Used for model trained with normalized data\n",
    "'''\n",
    "def MakeHML(model,std,mean):\n",
    "    def HML(x):\n",
    "        x = x.T\n",
    "        x = (x-mean)/std\n",
    "        return model.predict(x).T\n",
    "    return HML\n",
    "'''\n",
    "def MakeHML(model):\n",
    "    def HML(x):\n",
    "        return model.predict(x.T).T\n",
    "    return HML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML_exp(\n",
    "    RSEED,\n",
    "    sigmaobs,\n",
    "    nobs=1000\n",
    "    ):\n",
    "    X,Y,Y_noise = DataGen(nobs=nobs,RSEED=RSEED,sigmaobs=sigmaobs)\n",
    "\n",
    "    # Experiment with noised data\n",
    "\n",
    "    XX = np.transpose(X)\n",
    "    YY = np.transpose(Y_noise)\n",
    "\n",
    "    train_data,train_labels,test_data,test_labels = PrepareData(XX,YY)\n",
    "\n",
    "    model1 = build_model()\n",
    "\n",
    "    _,b,c=ML(train_data,train_labels,test_data,test_labels,\n",
    "             model=model1,\n",
    "             EPOCHS=EPOCHS,\n",
    "             MLDEBUG=False,\n",
    "             MLPLOTTING=False)\n",
    "    \n",
    "    print('.',end='')\n",
    "    \n",
    "    return b,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..."
     ]
    }
   ],
   "source": [
    "def main(RSEED_range,sigmaobs_range):\n",
    "    RSEED_num = len(RSEED_range)\n",
    "    sigmaobs_num = len(sigmaobs_range)\n",
    "\n",
    "    mae_arr = np.zeros([RSEED_num,sigmaobs_num])\n",
    "    acc_arr = np.zeros([RSEED_num,sigmaobs_num])\n",
    "\n",
    "    for i in range(RSEED_num):\n",
    "        for j in range(sigmaobs_num):\n",
    "            mae_arr[i,j],acc_arr[i,j] = ML_exp(RSEED_range[i],sigmaobs_range[j])\n",
    "\n",
    "    mae_sigmaobs = np.mean(mae_arr,axis=0)\n",
    "    acc_sigmaobs = np.mean(acc_arr,axis=0)\n",
    "    \n",
    "    return mae_sigmaobs,acc_sigmaobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RSEED_range = range(215,226)\n",
    "sigmaobs_range = np.linspace(0,1,11)\n",
    "mae_sigmaobs, acc_sigmaobs = main(RSEED_range,sigmaobs_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "if __name__==\"__main__\":\n",
    "    main()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
